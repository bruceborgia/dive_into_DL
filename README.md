# dive_into_DL

2023.10

### 1.softmax

$$
y_i= \frac{e^i}{\sum e^i}
$$

### 2.交叉熵损失函数

对单个样本，假设真实分布为y，网络输出分布为 $\widehat{y}_i$，总的类别数为n，则在这种情况下，交叉熵损失函数的计算方法为：

$$
Loss=−\sum^n_{i=1}y_ilog\widehat{y}_i
$$

```
用一个例子来说明，在手写数字识别任务中，如果样本是数字“5”，那么真实分布应该为：[ 0, 0, 0, 0, 0, 1, 0, 0, 0, 0 ]，
如果网络输出的分布为：[ 0.1, 0.1, 0, 0, 0, 0.7, 0, 0.1, 0, 0 ]，则n应为10，那么计算损失函数得：Loss=−0×log0.1−0×log0.1−0×log0−0×log0−0×log0−1×log0.7−0×log0−0×log0.1−0×log0−0×log0≈0.3567
如果网络输出的分布为：[ 0.2, 0.3, 0.1, 0, 0, 0.3, 0.1, 0, 0, 0 ]，那么计算损失函数得：Loss=−0×log0.2−0×log0.3−0×log0.1−0×log0−0×log0−1×log0.3−0×log0.1−0×log0−0×log0−0×log0≈1.2040
上述两种情况对比，第一个分布的损失明显低于第二个分布的损失，说明第一个分布更接近于真实分布，事实也确实是这样。
```

### 3.Dropout

希望       X的期望      E[X'] = X

$$
x'_i = 
\begin{cases}
    0 & \text{ 概率为 } p \\
    \frac{h}{1-p} & \text{ 其他情况}
\end{cases}
$$

### 4.BN作用于卷积，dropout作用于全连接层

### 5.卷积层

```
无padding时，形状变为
```



$$
(n_h - k_h + 1) \times (n_w - k_w + 1)
$$


```
填充padding, 卷积完成后，矩阵变为
```


```
```


$$
(n_h - k_h + p_h + 1) \times (n_w - k_w + p_w + 1)
$$


```
通常
```

$p_h = k_h - 1$, $p_w = k_w - 1 $ ,当$k_h$ 为奇数：在上下两侧填充$p_h / 2$

```
当
```

$k_h$为偶数：在上侧填充$\lceil k/2 \rceil$,下侧填充$\lfloor k/2 \rfloor$

```
```

**步幅**

```
给定高度
```

$s_h$ 和宽度$s_w$步幅，输出形状为：

$$
\lfloor \frac{(n_h - k_h + p_h + s_h)}{s_h} \rfloor \times \lfloor\frac{(n_w - k_w + p_w + s_w)}{s_w} \rfloor
$$

如果$p_h = k_h - 1, p_W = k_w - 1$,则

$$
\lfloor \frac{(n_h + s_h - 1)}{s_h} \rfloor \times \lfloor\frac{(n_w + s_w - 1)}{s_w} \rfloor
$$

如果输入X的高度和宽度能被步幅整除，则

$$
(\frac{n_h}{s_h}) \times(\frac{n_w}{s_w})
$$

### 6.Batch Normalization

```
bn层用于卷积和激活函数之间，加速收敛
```


### 7.锚框

```
bounding box 真实框
```


```
anchor 锚框
```


```
交并比iou
```


$$
IOU = \frac{A\cap B}{A \cup B}
$$

非极大抑制NMS：

```
1.选出置信度最高的预测框R1
```


```
2.选出R1与其他预测框的IOU值
```


```
3.取出IOU值高于设定阈值的预测框
```


```
4.从剩余预测框中选择下一个置信度最高的框
```


###　8.转置卷积

设高宽为n, 核为k, 填充为p, 步幅为s

转置卷积后，size变为： w = sn + k - 2p - s

$x = \begin{bmatrix}x_{11}&x_{12}&x_{!2}\\x_{21}&x_{22}&x_{23}\\x_{31}&x_{32}&x_{33}\end{bmatrix}  $                                         $w = \begin{bmatrix}w_{11}&w_{12}\\w_{21}&w_{22}\end{bmatrix} $

x展开并转置后为 $x^T = \begin{bmatrix}x_{11}\\x_{12}\\x_{13}\\x_{21}\\x_{22}\\x_{23}\\x_{31}\\x_{32}\\x_{33}\end{bmatrix} $

将w转换成稀疏矩阵		$w = \begin{bmatrix}w_{11}&w_{12}&0&w_{21}&w_{22}&0&0&0&0\\0&w_{11}&w_{12}&0&w_{21}&w_{22}&0&0&0\\0&0&0&w_{11}&w_{12}&0&w_{21}&w_{22}&0\\0&0&0&0&w_{11}&w_{12}&0&w_{21}&w_{22}\end{bmatrix}$

$y = w ·x^T =\begin{bmatrix}w_{11}·x_{11} +w_{12}·x_{12}+0+w_{21}·x_{21}+w_{22}·x_{22} + 0 + 0 + 0 + 0 \\0+w_{11}·x_{12} +w_{12}·x_{13}+0+w_{21}·x_{22}+w_{22}·x_{23}+0+0+0 \\0+0+0+w_{11}·x_{21} +w_{12}·x_{22}+0+w_{21}·x_{31}+w_{22}·x_{32}+0\\0+0+0+0+w_{11}·x_{22} +w_{12}·x_{23}+0+w_{21}·x_{32}+w_{22}·x_{33}\end{bmatrix}$ size = 4x1 $->$2x2

卷积时： y = C·X

转置卷积：X = $C^T$y

只是size一致， 无法还原X原来的值50-f5055gggggg50-wang
