# dive_into_DL

2023.10

### 1.softmax

$$
y_i= \frac{e^i}{\sum e^i}
$$



### 2.交叉熵损失函数

对单个样本，假设真实分布为y，网络输出分布为 $\widehat{y}_i$，总的类别数为n，则在这种情况下，交叉熵损失函数的计算方法为：
$$
Loss=−\sum^n_{i=1}y_ilog\widehat{y}_i
$$

​	用一个例子来说明，在手写数字识别任务中，如果样本是数字“5”，那么真实分布应该为：[ 0, 0, 0, 0, 0, 1, 0, 0, 0, 0 ]，
如果网络输出的分布为：[ 0.1, 0.1, 0, 0, 0, 0.7, 0, 0.1, 0, 0 ]，则n应为10，那么计算损失函数得：Loss=−0×log0.1−0×log0.1−0×log0−0×log0−0×log0−1×log0.7−0×log0−0×log0.1−0×log0−0×log0≈0.3567
如果网络输出的分布为：[ 0.2, 0.3, 0.1, 0, 0, 0.3, 0.1, 0, 0, 0 ]，那么计算损失函数得：Loss=−0×log0.2−0×log0.3−0×log0.1−0×log0−0×log0−1×log0.3−0×log0.1−0×log0−0×log0−0×log0≈1.2040
上述两种情况对比，第一个分布的损失明显低于第二个分布的损失，说明第一个分布更接近于真实分布，事实也确实是这样。

### 3.Dropout

希望       X的期望      E[X'] = X
$$
x'_i = 
\begin{cases}
    0 & \text{ 概率为 } p \\
    \frac{h}{1-p} & \text{ 其他情况}
\end{cases}
$$
4.BN作用于卷积，dropout作用域全连接层